{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "name": "sentiment-fine-tuning-huggingface.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zy4kamu/Coda/blob/master/sentiment_fine_tuning_huggingface.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-i247oQ0l4cq",
        "outputId": "ae3e4cf1-d84e-4c23-ea14-5fbe11653e66",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Mount Google Drive device to get access to train and validate datasets\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "train_file = \"/content/drive/My Drive/classification_train\"\n",
        "validate_file = \"/content/drive/My Drive/classification_validate\""
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "633fetsKg5cv",
        "outputId": "5b046dc2-c8c7-4626-aafc-06941577fb6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 558
        }
      },
      "source": [
        "# Install HuggingFace transformers\n",
        "\n",
        "!pip install git+https://github.com/huggingface/transformers.git"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+https://github.com/huggingface/transformers.git\n",
            "  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-umwph6u1\n",
            "  Running command git clone -q https://github.com/huggingface/transformers.git /tmp/pip-req-build-umwph6u1\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied (use --upgrade to upgrade): transformers==3.3.1 from git+https://github.com/huggingface/transformers.git in /usr/local/lib/python3.6/dist-packages\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (0.1.91)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (1.18.5)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (0.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (20.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (0.0.43)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (2019.12.20)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc2 in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (0.8.1rc2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (4.41.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==3.3.1) (3.0.12)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.3.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.3.1) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.3.1) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==3.3.1) (2.10)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.3.1) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers==3.3.1) (2.4.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.3.1) (0.16.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==3.3.1) (7.1.2)\n",
            "Building wheels for collected packages: transformers\n",
            "  Building wheel for transformers (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for transformers: filename=transformers-3.3.1-cp36-none-any.whl size=1082350 sha256=27e5636339e7b0f9a83baf53095503d83155cc71205909b221114894817657c9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-n7kvv05d/wheels/33/eb/3b/4bf5dd835e865e472d4fc0754f35ac0edb08fe852e8f21655f\n",
            "Successfully built transformers\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9pi31_2cndZU"
      },
      "source": [
        "# Import required packages\n",
        "\n",
        "from transformers import DistilBertTokenizerFast\n",
        "from transformers import TFDistilBertForSequenceClassification\n",
        "\n",
        "import tensorflow as tf\n",
        "import json"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "UZsRQ-N8ndZa",
        "outputId": "16b04641-adb2-418c-9675-90d4201a0a2e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        }
      },
      "source": [
        "# Read training and validate datasets\n",
        "\n",
        "def read_dataset(input_file):\n",
        "  sentences = []\n",
        "  labels = []\n",
        "  with open(input_file) as reader:\n",
        "    train_json = json.load(reader)\n",
        "    for item in train_json:\n",
        "      sentences.append(item['text'])\n",
        "      labels.append(item['label'])\n",
        "  return sentences, labels \n",
        "\n",
        "training_sentences, training_labels = read_dataset('/content/drive/My Drive/classification_train')\n",
        "validation_sentences, validation_labels = read_dataset('/content/drive/My Drive/classification_validate')\n",
        "\n",
        "print('Training sentences:', training_sentences[0:3], '...')\n",
        "print('Training labels:', training_labels[0:3], '...')\n",
        "print('Validation sentences:', validation_sentences[0:3], '...')\n",
        "print('Validation labels:', validation_labels[0:3], '...')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training sentences: ['play a jack lawrence concerto', 'play some sam moore.', 'add album to princesas indie'] ...\n",
            "Training labels: [6, 6, 5] ...\n",
            "Validation sentences: ['where is belgium located', 'let me hear the good songs from james iha', 'play something by duke ellington from the seventies'] ...\n",
            "Validation labels: [0, 6, 6] ...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "KCxtcxObndZk"
      },
      "source": [
        "# Create training and validation datasets for TensorFlow backend\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')\n",
        "\n",
        "train_encodings = tokenizer(training_sentences,\n",
        "                            truncation=True,\n",
        "                            padding=True)\n",
        "val_encodings = tokenizer(validation_sentences,\n",
        "                            truncation=True,\n",
        "                            padding=True)\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    training_labels\n",
        "))\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(val_encodings),\n",
        "    validation_labels\n",
        "))"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "8_gjepLSndZq",
        "outputId": "00e808d6-c845-4ca7-d2fe-7c5cc4259c00",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "# Train the model\n",
        "\n",
        "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased',\n",
        "                                                              num_labels=7)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "model.compile(optimizer=optimizer, loss=model.compute_loss, metrics=['accuracy'])\n",
        "model.fit(train_dataset.shuffle(100).batch(16),\n",
        "          epochs=3,\n",
        "          batch_size=16,\n",
        "          validation_data=val_dataset.shuffle(100).batch(16))\n",
        "model.save_pretrained('/tmp/classification_model')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing TFDistilBertForSequenceClassification: ['vocab_layer_norm', 'vocab_projector', 'activation_13', 'vocab_transform']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier', 'dropout_99', 'pre_classifier']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "862/862 [==============================] - 77s 89ms/step - loss: 0.1397 - accuracy: 0.9684 - val_loss: 0.0428 - val_accuracy: 0.9943\n",
            "Epoch 2/3\n",
            "862/862 [==============================] - 76s 88ms/step - loss: 0.0378 - accuracy: 0.9896 - val_loss: 0.0434 - val_accuracy: 0.9871\n",
            "Epoch 3/3\n",
            "862/862 [==============================] - 76s 88ms/step - loss: 0.0236 - accuracy: 0.9941 - val_loss: 0.0539 - val_accuracy: 0.9857\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "dmfeNn8hndZs"
      },
      "source": [
        ""
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "pycharm": {
          "name": "#%%\n"
        },
        "id": "akGTf-l_ndZy",
        "outputId": "89db843a-b69a-4a34-a5f0-e17214c1958e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        }
      },
      "source": [
        "# Load model and make prediction on specific sentence \n",
        "\n",
        "loaded_model = TFDistilBertForSequenceClassification.from_pretrained('/tmp/classification_model')\n",
        "test_sentence = 'play a jack lawrence concerto'\n",
        "\n",
        "predict_input = tokenizer.encode(test_sentence,\n",
        "                                 truncation=True,\n",
        "                                 padding=True,\n",
        "                                 return_tensors='tf')\n",
        "\n",
        "tf_output = tf.nn.softmax(loaded_model.predict(predict_input)[0]).numpy()[0]\n",
        "\n",
        "labels = ['GetWeather', 'SearchCreativeWork', 'RateBook', 'SearchScreeningEvent',\n",
        "          'BookRestaurant', 'AddToPlaylist', 'PlayMusic']\n",
        "print()\n",
        "for label, probability in zip(labels, tf_output):\n",
        "  print('{}: {:0.5f}'.format(label, probability))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at /tmp/classification_model were not used when initializing TFDistilBertForSequenceClassification: ['dropout_99']\n",
            "- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at /tmp/classification_model and are newly initialized: ['dropout_359']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:11 out of the last 11 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7f3a4c6341e0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/tutorials/customization/performance#python_or_tensor_args and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "\n",
            "GetWeather: 0.00004\n",
            "SearchCreativeWork: 0.00052\n",
            "RateBook: 0.00004\n",
            "SearchScreeningEvent: 0.00008\n",
            "BookRestaurant: 0.00008\n",
            "AddToPlaylist: 0.00015\n",
            "PlayMusic: 0.99909\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}